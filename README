# Holophrasm: a purely neural automated theorem prover for Metamath

Holophrasm is an automated theorem prover for the Metamath formal
mathematics verification system that uses deep learning.
It was originally developed by Daniel Whalen.
This version was modified by David A. Wheeler.

For technical details, see:
"Holophrasm: a purely neural automated theorem prover for higher-order logic"
by Daniel Whalen, 10 Aug 2016, https://arxiv.org/abs/1608.02644

In the longer term we hope to make the whole process of installing
and using this program easier, including having a front-end command line
program named "holophrasm" that makes it all work together.
That isn't here yet, so here document how to get things working right now.

## Installation: Program

You need to get the program itself (reading this README), and
change into that directory:

~~~~
git clone https://github.com/david-a-wheeler/holophrasm
cd holophrasm
~~~~

Install system packages.  This must include Python2 (if not already installed), including its "development" packages (typically called python2-dev or python2-devel, though it might say "python" instead of "python2").  You must also install a working C compiler such as gcc (this should automatically happen when you install the python-dev/python-devel package).

If you're using Cygwin, you need to separately install pip (the Python2 package manager) by running:

~~~~
python2 -m ensurepip
pip2 install --upgrade pip
~~~~

If you're using Debian/Ubuntu or similar, you need to separately install pip by running:

~~~~
sudo apt install python-pip
~~~~

Now install the Python language dependencies (NumPy and iPython):

~~~~
pip2 install numpy ipython
~~~~

## Installation: Data files

To use the program, we also need to download the original release by
Daniel Whalen, which includes the binary files for the language model
and the trained model weights.  We'll use wget here, but
anything that downloads holophrasm.zip will do:

~~~~
wget https://github.com/dwhalen/holophrasm/releases/download/v1.0/holophrasm.zip
~~~~

*BEWARE* - if you unzip this, it will unzip to a "holophrasm" directory
and you don't want to overwrite the holophrasm directory you just
got from GitHub.  The easy solution is to make sure you are ALREADY
in the holophrasm directory, and put the .zip file INSIDE it
(as we suggest).  Now unzip it, and rename the directory to
prevent confusion (on Windows use "rename" instead of "mv", as well
as whatever unzip program you want):

~~~~
unzip holophrasm.zip
mv holophrasm holophrasm-release
~~~~

Now copy the the binary files for the language model
and the trained model weights:

# Download "release" file and copy its big files:
mv holophrasm-release/lm .
mv holophrasm-release/searcher/gen.weights searcher/
mv holophrasm-release/searcher/payout.weights searcher/
mv holophrasm-release/searcher/pred.weights searcher/
mv holophrasm-release/searcher/pred_database searcher/
mkdir searcher/proofs

Note that these data files total about 1134 MB (base-10).

## Usage

Then, from the root directory, using python 2.7, run

~~~~
python2 run_script_ordered.py
python2 write_all_proofs.py
~~~~

run_script_ordered.py will attempt to search for proofs for all the Metamath propositions in the test set, starting with the (expected) easiest proofs first.  Reasonable parameters for running the script are:
timeout: 5
num_passes: 10000
search beam width: 20
hyp bonus: 0
Proofs are saved to searcher/proofs as soon as they are found.  The proofs that I found during my search are in searcher/proofs_baseline.

write_all_proofs.py will generate a modified_set.mm module file, which consists of the proofs from set.mm, with the found proofs replaced.  This can then be fed into Metamath and verified using the commands

~~~~
READ modified_set.mm
VERIFY PROOF *
~~~~

There's a known bug involving outputting the proofs that assigns the same additional dummy variable to multiple set variables, breaking the disjointness condition.
I think this is purely a problem with the proof writing, but I'm not confident, so I've excluded these proofs from my count of working proofs anyway.

## Training

If you want to train from scratch, you can do the following.  Daniel Whalen
recommends having at least 64GB of RAM.

Training is done against a particular mm file (currently the constant "set.mm",
though in the future we want it to be more flexible).
A number of the training steps below invoke
meta_math_database defined in tree_parser.py, which reads the file
as part of its processing.

First, install the additional required libraries.
On Debian/Ubuntu:

~~~~
sudo apt install python-cpuinfo python-matplotlib
~~~~

Step 0: (OS X only) deal with Accelerate and forking bug

~~~~
export VECLIB_MAXIMUM_THREADS=1
~~~~

Step 1: train gen and pred

Each of these scripts run indefinitely, trying to find a better
training result for gen and pred respectively.  So run each for a while
and then use a keyboard interrupt to halt their execution,
preferably after the training rate decays to a sufficiently low value.
All of these scripts may be run in parallel.


~~~~
python3 script_gen.py # Train the generative network
python3 script_pred.py # Train the prediction network
python3 save_language_model.py # Save a copy of the language model somewhere
~~~~

Step 2: set up the files for the nn interface

~~~~
mkdir -p searcher
cp weights/gen2000/train.parameters searcher/gen.parameters
cp weights/gen2000/train.weights searcher/gen.weights
cp weights/pred2000/train.parameters searcher/pred.parameters
cp weights/pred2000/train.weights searcher/pred.weights
~~~~

TODO: Is the following still true?:

You also need to save a copy of the language model somewhere by running

~~~~
python3 save_language_model.py
~~~~

Step 3: construct the payout data set and train the payout network
The first script takes days, but is highly parallelizable. Chunks may be run on
different machines, and multiprocessing can be enabled by changing the
"withpool.Pool(None)" to "withpool.Pool(n)", where n is the desired number of
processes.
WARNING: multiprocessing support may crash Windows machines.

This will report "included XXX tokens" and show
"proposition: YYY" repeatedly as they are saved.

Step 4: construct the payout data set and train the payout network

~~~~
python3 generate_payout_data_script.py
python3 script_payout.py
cp weights/pred/train.parameters searcher/payout.parameters
cp weights/pred/train.weights searcher/payout.weights
~~~~

We are now done with training.
The following steps show how to use the trained data.

Step 5: run the proof search (hyp bonus should be set to 0.0 for consistency with the paper)
The search will be much faster if you set "multi=True" in the searcher.run call, but this
may break on Windows.

~~~~
python3 run_script_ordered.py
~~~~

Step 6: verify the proofs

~~~~
python3
import write_proof
write_proof.reset()
write_proof.write_all_known_proofs()
~~~~

Step 7: copy the modified_set.mm into the metamath folder and use the following metamath commands to verify the proofs

~~~~
ERASE
READ modified_set.mm
VERIFY PROOF *
~~~~
